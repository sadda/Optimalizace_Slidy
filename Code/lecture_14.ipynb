{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lekce 14: Nelineární metody nejmenších čtverců"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nejdříve načtěme nutné balíčky a funkce z minulé hodiny."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Random\n",
    "using LinearAlgebra\n",
    "using Plots\n",
    "\n",
    "include(\"utilities.jl\")\n",
    "\n",
    "function grad_descent(grad, x; α=1e-1, max_iter=100, ϵ_tol=1e-6)\n",
    "    res = zeros(max_iter)\n",
    "    x_all = zeros(length(x), max_iter)\n",
    "    for i in 1:max_iter\n",
    "        x_all[:,i] .= x\n",
    "        x -= α*grad(x)\n",
    "        res[i] = norm(grad(x))\n",
    "        if norm(grad(x)) <= ϵ_tol\n",
    "            res = res[1:i]\n",
    "            x_all = x_all[:,1:i]\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    return x, x_all, res\n",
    "end\n",
    "\n",
    "function newton(grad, hess, x; max_iter=100, ϵ_tol=1e-12)\n",
    "    res = zeros(max_iter)\n",
    "    x_all = zeros(length(x), max_iter)\n",
    "    for i in 1:max_iter\n",
    "        x_all[:,i] .= x\n",
    "        x -= hess(x) \\ grad(x)\n",
    "        res[i] = norm(grad(x))\n",
    "        if norm(grad(x)) <= ϵ_tol\n",
    "            res = res[1:i]\n",
    "            x_all = x_all[:,1:i]\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    return x, x_all, res\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Nejmenší čtverce"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Úkolem dnešní hodiny bude nafitovat $n$ dvojic vzorků $(x_i, y_i)$ pomocí lineárních a nelineárních funkcí. Data $x_i$ budou rovnoměrně rozdělené na intervalu $[-2,2]$ a budeme uvažovat přesnou závislost $$y_i = h_{\\rm true}(x_i) = \\sin x_i - 0.1x_i + 1.$$ Důležité je si uvědomit, že tato funkce je neznámá, a tedy ji nemůžeme použít pro transformaci dat $x_i$. Vytvořme nejdříve data a vykresleme je."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "h_true(x) = sin(x) - 0.1x + 1\n",
    "\n",
    "n = 1000\n",
    "xs = range(-2, 2; length=n)\n",
    "ys = h_true.(xs)\n",
    "\n",
    "plot(xs, ys, label=\"Data\", legend=:topleft)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Při řešení se parametrizuje prostor hledaných predikcí pomocí nějaké funkce $\\text{predict}(w;x)$. Zde je důležité si uvědomit rozdíl mezi parametry: zatímco $x$ jsou vstupní data, $w$ jsou parametry, které budeme optimalizovat. Poté řešíme optimalizační úlohu $$\\text{minimalizuj}_w\\qquad \\frac{1}{2n}\\sum_{i=1}^n (\\text{predict}(w;x_i) - y_i)^2$$ přes všechny možné parametry $w$. Chceme tedy minimalizovat vzdálenost mezi predikcí $\\text{predict}(w;x_i)$ a labelem $y_i$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nejprve zadefinujme nějakou obecnou predikční funkci $\\text{predict}$, která se bude snažit aproximovat $h_{\\rm true}$. Funkce $\\text{predict}$ samozřejmě závisí na datech $x$, ale zároveň musí záviset na nějakých parametrech $w$, která budeme trénovat. Zadefinujeme dvě predikční funkce, lineární $$\\text{predict}(w,x)=w_1x+w_2$$ a nelineární $$\\text{predict}(w,x)=w_1\\sin w_2x + w_3\\cos w_4x + w_5x+ w_6.$$ První povede na lineární nejmenší čtverce, což se na přednášce dělalo několik týdnů zpátky. Zadefinujme tyto dvě funkce a spočtěme jejich derivace."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "predict_lin(w,x) = w[1]*x + w[2]\n",
    "predict_lin_grad(w,x) = [x 1]\n",
    "\n",
    "predict_nonlin(w,x) = w[1]*sin(w[2]*x) + w[3]*cos(w[4]*x) + w[5]*x + w[6]\n",
    "predict_nonlin_grad(w,x) = [sin(w[2]*x) x*w[1]*cos(w[2]*x) cos(w[4]*x) -x*w[3]*sin(w[4]*x) x 1];"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nyní zadefinujme funkce se stejným značením jako na přednášce. Protože $$g_i(w) = \\text{predict}(w,x_i) - y_i$$ ukazuje chybu při fitu i-tého vzorku, $$f(w) = \\frac {1}{2n}\\sum_{i=1}^n g_i(w)^2$$ ukazuje průměrnou kvadratickou chybu přes všechny vzorky. Je důležité si uvědomit, že proměnná $x$ už označuje vstupní data, a tedy pro optimalizovanou proměnnou jsme použili písmeno $w$. Nyní tyto funkce zadefinujeme. Zároveň spočteme gradienty $f$ i $g$."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "g(w) = [predict(w,x) - y for (x,y) in zip(xs,ys)]\n",
    "g_grad(w) = vcat([predict_grad(w,x) for x in xs]...)\n",
    "\n",
    "f(w) = g(w)'*g(w) / (2*length(g(w)))\n",
    "f_grad(w) = g_grad(w)'*g(w) / length(g(w))\n",
    "\n",
    "f(x::Real,y::Real) = f([x,y]);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "V lineárních i nelineárních čtvercích chceme minimalizovat funkci $f$ a oba přístupy se liší pouze tím, jak je definovaná funkce $\\text{predict}$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lineární nejmenší čtverce"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pro lineární nejmenší čtverce definujme lineární predikci."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "predict = predict_lin\n",
    "predict_grad = predict_lin_grad;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lineární nejmenší čtverce minimalizují funkci $f$. Vykresleme tedy její vrstevnice. Znovu si uvědomme, že optimalizujeme přes proměnnou $w$."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "w1lim = range(0, 1; length=31)\n",
    "w2lim = range(0, 2; length=31)\n",
    "\n",
    "contourf(w1lim, w2lim, f; color=:jet)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lineární nejmenší čtverce mají řešení v uzavřené formě $w=(A^\\top A)^{-1}A^\\top y$. Když toto řešení spočteme a vykreslíme, není překvapivé, že se nachází v minimum funkce $f$."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "A = hcat(xs, ones(n))\n",
    "w_opt1 = (A'*A) \\ (A'*ys)\n",
    "\n",
    "contourf(w1lim, w2lim, f; color=:jet)\n",
    "scatter!([w_opt1[1]], [w_opt1[2]]; label=\"Optimum\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pro řešení jsme použili neznámý zápis `(A'*A) \\ (A'*ys)`. Tento příkaz dá stejný výsledek jako `inv(A'*A)*A'*y`. Rozdíl mezi nimi je ten, že zatímco první příkaz používá specializované algoritmy pro řešení rovnic, druhý nejdrív spočte inverzi matice a teprve potom ji vynásobí vektorem. Tyto rozdíly budeme více komentuje na konci souboru."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Z minulé hodiny máme naprogramovaný gradient descent. Pustíme ho tedy stejně jako minule."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "w0 = [0;0]\n",
    "\n",
    "w_opt2, w_all2, res2 = grad_descent(f_grad, w0);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dostali jsme optimální parametry, ale zajímá nás predikce. Tu dostaneme jako $w_1x+w_2$. Po vykreslení dostaneme nejlepší lineární aproximaci, která ale není moc dobrá."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(xs, ys, label=\"Data\", legend=:topleft)\n",
    "plot!(xs, x -> predict(w_opt2, x), label=\"Fit\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Použijme opět stejnou funkci jako na minulé hodině a vykresleme konvergenci iterací."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "create_anim(f, w_all2, w1lim, w2lim, \"Anim_NC1.gif\");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](Anim_NC1.gif)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Metody založené na Newtonově metodě"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pro použití Gauss-Newtonovy and Levenberg-Marquardtovy metody je dobré si uvědomit, že obě pracují stejně jako Newtonova metoda, tedy krok je $-A^{-1}\\nabla f(x)$ pro nějakou matici $A$. Pro Newtonovu metodu se za $A$ bere Hessián, zatímco pro dvě výše zmíněné metoda to je nějaká jeho aproximace. Není tedy nutné psát novou optimalizační funkci, ale stačí použit již napsanou funkci `newton` se správným vstupem `h` druhých derivací. Tyto derivace jde spočíst následovně:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "f_hess_approx(w,λ) = g_grad(w)'*g_grad(w) / length(g(w)) + λ*Diagonal(ones(length(w)));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gauss-Newtonova metoda používá `μ=0`, zatímco Levenberg-Marquardtova metoda používá `μ>0`. Poté již stačí zavolat Newtonovu metodu a dostaneme řešení."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "w_opt3, w_all3, res3 = newton(f_grad, w -> f_hess_approx(w, 0), w0)\n",
    "w_opt4, w_all4, res4 = newton(f_grad, w -> f_hess_approx(w, 0.5), w0);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Je dobré si uvědomit, že Gauss-Newtonova metoda je přesná Newtonova metoda, neboť druhá derivace `g` je nulová. Vzhledem k tomu, že `f` je kvadratická, pro `w_opt3` dostáváme konvergenci v jedné iteraci. Pro Levenberg-Marquardtovu metodu je konvergence pomalejší. Navíc perturbace Hessiánu změnila superlineární (rychlou) konvergenci na pouhou lineární konvergenci."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(res4; yscale=:log10, label=\"Residual\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nakonec opět vykresleme jednotlivé iterace."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "create_anim(f, w_all4, w1lim, w2lim, \"Anim_NC2.gif\");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](Anim_NC2.gif)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Stochastický gradient descent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pro stochastický gradient descent definujme funkce stejně jako v první části přednášky, ale uvažujme pouze vzorky v nějaké indexové množině $I$. To odpovídá tomu, že uvažujeme pouze ty řádky matice $A$, které odpovídají těmto indexům."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "g(w, I) = A[I,:]*w - ys[I]\n",
    "g_grad(w, I) = A[I,:]\n",
    "\n",
    "f(w, I) = g(w, I)'*g(w, I) / (2*length(g(w, I)))\n",
    "f_grad(w, I) = g_grad(w, I)'*g(w, I) / length(g(w, I));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stochastický gradient descent je stejný jako standardní gradient descent, ale gradient počítáme pouze ze zmenšeného počtu vzorků. Největší výhoda stochastického gradientu je rychlost počítání, neboť se pracuje pouze s malým počtem vzorků."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function stoch_grad_descent(grad, x, n; n_minibatch=8, α=1e-1, max_iter=100)\n",
    "    res1 = zeros(max_iter)\n",
    "    res2 = zeros(max_iter)\n",
    "    x_all = zeros(length(x), max_iter)\n",
    "    for i in 1:max_iter\n",
    "        x_all[:,i] = x\n",
    "        I = randperm(n)[1:n_minibatch]\n",
    "        x -= α*grad(x, I)\n",
    "        res1[i] = norm(grad(x, I))\n",
    "        res2[i] = norm(grad(x, 1:n))\n",
    "    end\n",
    "    return x, x_all, res1, res2\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pusťme nyní stochastický gradient descent."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "w_opt5, w_all5, res5_1, res5_2 = stoch_grad_descent(f_grad, w0, n);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dále pak vykresleme rezidua na minibatchi a na celém souboru. Vidíme, že rezidua na minibatchi velmi skáčou. Důvod je ten, že minibatch je malý, což přidává nestabilitu."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(res5_1; yscale=:log10, label=\"Residual minibatch\")\n",
    "plot!(res5_2; yscale=:log10, label=\"Residual true\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Iterace konvergují zpočátku rychle, ale když se dostanou poblíž řešení, tak začnou skákat. Důvodem je opět malá velikost minibatche. Pro konvergenci by bylo potřeba snižovat délku kroku."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "create_anim(f, w_all5, w1lim, w2lim, \"Anim_NC3.gif\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](Anim_NC3.gif)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I když jsme nedokonvergovali, následující obrázek ukazuje, že jsme pořád blízko dobrého řešení."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(xs, ys, label=\"Data\", legend=:topleft)\n",
    "plot!(xs, x -> predict(w_opt2, x), label=\"Fit: Optimální\")\n",
    "plot!(xs, x -> predict(w_opt5, x), label=\"Fit: SGD\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Nelineární nejmenší čtverce"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nyní aktivujme nelineární predikci."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "predict = predict_nonlin\n",
    "predict_grad = predict_nonlin_grad;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zvolme počáteční bod jedniček a stejně jako v lineárním případě pusťme Levenberg-Marquardtovu metodu s parameterem $\\lambda=0.001$"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "w0 = ones(6)\n",
    "\n",
    "w_opt6, w_all6, res6 = newton(f_grad, w -> f_hess_approx(w, 0.001), w0);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Po zaokrouhlední jsme dostali predikční funkci $$0.99\\sin(1x) + 0.78\\cos(0x) - 0.1x + 0.22 = 0.99\\sin(x) - 0.1x + 1,$$ což je skoro perfektní fit. Vykresleme nyní tento fit. Data a fit jsou skoro identické."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(xs, ys, label=\"Data\", legend=:topleft)\n",
    "plot!(xs, x -> predict(w_opt6, x), label=\"Fit\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neuronové sítě"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nevýhoda předchozího přístupu je, že musíme přesně parametrizovat funkci `predict`. Ukážeme si nyní, jak nafitovat onu sinusoidu pomocí jednoduché neuronové sítě, kde tato parametrizace není nutná. Neuronová síť není nic jiného než nelineární zobrazení s nějakým speciálním předpisem. Načtěme nejdříve nutné balíky."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Flux\n",
    "using Flux: mse\n",
    "using Base.Iterators: partition"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vzhledem k tomu, že balík Flux vyžaduje, aby poslední dimenze vstupů byly vzorky, tak musíme vstupná data transformovat do řádkového vektoru. Zároveň je kvůli rychlostem výpočtu konvertujeme z `Float64` do `Float32`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "xs_row = Float32.(reshape(xs,1,:))\n",
    "ys_row = Float32.(reshape(ys,1,:));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nyní zkonstruujme jednoduchou neuronovou síť s dvěma skrytými vrstvami."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "n_hidden = 10\n",
    "m = Chain(\n",
    "    Dense(1, n_hidden, relu),\n",
    "    Dense(n_hidden, n_hidden, relu),\n",
    "    Dense(n_hidden, 1),\n",
    ");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zadefinujme účelovou funkci jako mean square error, vytáhněme ze sítě parametry (které na začátku byly označeny jako $w$) a jako optimalizátor použijme gradient descent."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "loss(x, y) = mse(m(x), y)\n",
    "ps = params(m)\n",
    "opt = Descent(1e-1);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nyní udělejme 100 iterací gradient descentu. Všimněme si, že Flux automaticky počítá derivace a provádí update parametrů."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "max_iter = 100\n",
    "\n",
    "Ls1 = zeros(max_iter)\n",
    "for i in 1:max_iter\n",
    "    gs = gradient(ps) do\n",
    "        loss(xs_row, ys_row)\n",
    "    end\n",
    "\n",
    "    Flux.update!(opt, ps, gs)\n",
    "    Ls1[i] = loss(xs_row, ys_row)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Po vykreslení vidíme, že máme docela dobrý fit."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(xs, ys, label=\"Data\", legend=:topleft)\n",
    "plot!(xs, m(xs_row)[:], label=\"Fit\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nyní udělejme to samé, ale se stochastickým gradient descentem. Protože model `m` si v sobě nese optimalizované parametry, tak ho nejdříve znovu inicializujme."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "m = Chain(\n",
    "    Dense(1, n_hidden, relu),\n",
    "    Dense(n_hidden, n_hidden, relu),\n",
    "    Dense(n_hidden, 1),\n",
    ");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nyní udělejme iterátor, který všechny vzorky rozdělí do minibachů, kde každý minibatch má velikost 10 vzorků."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "batch_size = 10\n",
    "batches_train = map(partition(randperm(size(ys_row, 2)), batch_size)) do inds\n",
    "    return (xs_row[:, inds], ys_row[:, inds])\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pusťme stochastický gradient descent na 100 epoch. V jedné epoše by se optimalizátor měl podívat na každý vzorek právě jednou. Vzhledem k tomu, že máme 1000 dat a minibatch je velikosti 10, tak za 100 epoch uděláme 10000 gradientních updatů. Stochastický gradient tedy za stejný čas udělá daleko více updatů než gradient descent. I když jsou tyto updaty nepřesné, tak rychlostní bonus je většinou tak výrazný, že je dobré stochastickou verzi uvažovat."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Ls2 = zeros(max_iter)\n",
    "for i in 1:max_iter\n",
    "    Flux.train!(loss, params(m), batches_train, opt)\n",
    "    Ls2[i] = loss(xs_row, ys_row)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Porovnejme nyní běžný a stochastický gradient descent. Vidíme, že stochastická varianta má výrazně menší ztrátovou funkci."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(Ls1, label=\"GD\", xlabel=\"Iterace\", ylabel=\"Ztrátová funkce\", legend=:topleft, yscale=:log10)\n",
    "plot!(Ls2, label=\"SGD\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Když vykreslíme predikci, dsotáváme skoro perfektní fit. Do odpovídá ne úplně běžné situaci, že stochastický gradient descent dokonvergoval do globálního minima."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "plot(xs, ys, label=\"Data\", legend=:topleft)\n",
    "plot!(xs, x -> predict(w_opt6,x), label=\"Fit nonlinear\")\n",
    "plot!(xs, m(xs_row)[:], label=\"Fit neural\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zdálo by se, že všechno je růžové, ale co se stane, když vykreslíme fit mimo obor dat?"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "xs_ext = -10:0.01:10\n",
    "\n",
    "plot(xs_ext, h_true, label=\"Data\", legend=:topleft)\n",
    "plot!(xs_ext, x -> predict(w_opt6,x), label=\"Fit nonlinear\")\n",
    "plot!(xs_ext, m(Float32.(reshape(xs_ext,1,:)))[:], label=\"Fit neural\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Není vůbec dobrý. Toto je ale vlastnost všech modelů. Když učíme model na datech z intervalu $[-2,2]$ a pak ho testujeme mimo tento interval, nemůžeme očekávat, že tam bude fungovat dobře. Na drunou stranu fit pomocí parametrizované funkce je pořád dobrý."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Řešení soustavy lineárních rovnic"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vraťme se nyní k tomu, jak se od sebe liší zápisy `inv(A)*b` a `A\\b`. Vygenerujme náhodnou řídkou matici `aux_A1` a poté ji přetransformujme do husté matice."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using SparseArrays\n",
    "\n",
    "aux_s = 1000\n",
    "\n",
    "aux_A1 = sprandn(aux_s, aux_s, 0.001)\n",
    "aux_A1 += I\n",
    "aux_A2 = Matrix(aux_A1);\n",
    "aux_b = randn(aux_s);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Následující kód ukazuje, že `inv(aux_A2)*aux_b` a `aux_A2\\aux_b` dává stejný výsledek."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "norm(inv(aux_A2)*aux_b - aux_A2\\aux_b)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Udělejme nyní časové porovnání pomocí balíku `BenchmarkTools`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import BenchmarkTools: @btime\n",
    "\n",
    "println(\"Dense matrix based on inv(A)*b\")\n",
    "@btime inv($aux_A2) * $aux_b;\n",
    "\n",
    "println(\"Dense matrix based on A \\\\ b\")\n",
    "@btime $aux_A2 \\ $aux_b;\n",
    "\n",
    "println(\"Sparse matrix based on A \\\\ b\")\n",
    "@btime $aux_A1 \\ $aux_b;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vidíme, že syntaxe `A \\ b` je několikrát rychlejší a má menší nároky na paměť. Při použití řídké matice je rozdíl ještě markantnější, neboť `inv(A)` generuje hustou matici a není schopné využít řídkosti."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  },
  "kernelspec": {
   "name": "julia-1.5",
   "display_name": "Julia 1.5.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
