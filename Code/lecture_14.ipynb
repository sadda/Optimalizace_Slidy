{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lekce 14: Nelineární metody nejmenších čtverců"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nejdříve načtěme nutné balíčky a funkce z minulé hodiny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "using LinearAlgebra\n",
    "using Plots\n",
    "\n",
    "include(\"utilities.jl\")\n",
    "\n",
    "function grad_descent(grad, x; α=1e-1, max_iter=100, ϵ_tol=1e-6)\n",
    "    res = zeros(max_iter)\n",
    "    x_all = zeros(length(x), max_iter)\n",
    "    for i in 1:max_iter\n",
    "        x_all[:,i] .= x\n",
    "        x -= α*grad(x)\n",
    "        res[i] = norm(grad(x))\n",
    "        if norm(grad(x)) <= ϵ_tol\n",
    "            res = res[1:i]\n",
    "            x_all = x_all[:,1:i]\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    return x, x_all, res\n",
    "end\n",
    "\n",
    "function newton(grad, hess, x; max_iter=100, ϵ_tol=1e-12)\n",
    "    res = zeros(max_iter)\n",
    "    x_all = zeros(length(x), max_iter)\n",
    "    for i in 1:max_iter\n",
    "        x_all[:,i] .= x\n",
    "        x -= hess(x) \\ grad(x)\n",
    "        res[i] = norm(grad(x))\n",
    "        if norm(grad(x)) <= ϵ_tol\n",
    "            res = res[1:i]\n",
    "            x_all = x_all[:,1:i]\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    return x, x_all, res\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nejmenší čtverce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Úkolem dnešní hodiny bude nafitovat $n$ dvojic $(x_i, y_i)$ pomocí lineárních a nelineárních funkcí. Data $x_i$ budou rovnoměrně rozdělené na intervalu $[-2,2]$ a budeme uvažovat přesnou závislost $$y_i = \\sin x_i - 0.1x_i + 1.$$ Důležité je si uvědomit, že tato funkce je neznámá, a tedy ji nemůže použít pro transformaci dat $x_i$. Při řešení se parametrizuje prostor hledaných predikcí pomocí nějaké funkce $h(w;x)$. Zde je důležité si uvědomit rozdíl mezi parametry: zatímco $x$ jsou vstupní data, $w$ jsou hledané parametry. Poté řešíme optimalizační úlohu $$\\operatorname{minimalizuj}\\qquad \\frac 1n\\sum_{i=1}^n (h(w;x_i) - y_i)^2$$ přes všechny možné parametry $w$. Chceme tedy minimalizovat vzdálenost mezi predikcí $h(w;x_i)$ a labelem $y_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lineární nejmenší čtverce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vzhledem k tomu, že máme jednorozměrný vstup, pro lineární nejmenší čtverce máme $$h(w;x) = w_1x + w_2$$. Lineární nejmenší čtverce potom mají známý tvar $$\\operatorname{minimalizuj}\\qquad \\frac 1n\\sum_{i=1}^n (w_1x_i + w_2 - y_i)^2$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vytvořme nejdříve data a vykreleme je."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "xs = range(-2, 2; length=n)\n",
    "ys = sin.(xs) .- 0.1*xs .+ 1\n",
    "\n",
    "scatter(xs, ys, label=\"Data\", legend=:topleft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyní zadefinujme funkce se stejným značením jako na přednášce. Nejprve $$g_i(w) = w_1x_i + w_2 - y_i$$ ukazuje chybu při fitu i-tého pozorování. Poté $$f(w) = \\frac 1n\\sum_{i=1}^n g_i(w)^2$$ ukazuje průměrnou kvadratickou chybu přes všechny pozorování. Je důležité si uvědomit, že proměnná $x$ už označuje vstupní data, a tedy pro optimalizovanou proměnnou musíme použít jiné písmeno, například $w$. Nyní tyto funkce zadefinujeme. Použijeme maticový zápis s maticí `A`. Zároveň spočteme gradienty $f$ i $g$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = hcat(xs, ones(n))\n",
    "\n",
    "g(w) = A*w .- ys\n",
    "g_grad(w) = A\n",
    "\n",
    "f(w) = g(w)'*g(w) / (2*length(g(w)))\n",
    "f_grad(w) = g_grad(w)'*g(w) / length(g(w));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lineární nejmenší čtverce minimalizují funkci $f$. Vykresleme tedy její vrstevnice. Znovu si uvědomme, že optimalizujeme přes proměnnou $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1lim = range(0, 1; length=31)\n",
    "w2lim = range(0, 2; length=31)\n",
    "\n",
    "contourf(w1lim, w2lim, (w1,w2) -> f([w1;w2]); color=:jet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lineární nejmenší čtverce mají řešení v uzavřené formě $w=(A^\\top A)^{-1}A^\\top y$. Když toto řešení spočteme a vykreslíme, není překvapivé, že se nachází v minimum funkce $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_opt1 = (A'*A) \\ (A'*ys)\n",
    "\n",
    "contourf(w1lim, w2lim, (w1,w2) -> f([w1;w2]); color=:jet)\n",
    "scatter!([w_opt1[1]], [w_opt1[2]]; label=\"Optimum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Odbočka k řešení soustavy lineárních čtverců"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pro řešení jsme použili neznámý zápis `(A'*A) \\ (A'*ys)`. Tento příkaz dá stejný výsledek jako `inv(A'*A)*A'*y`. Rozdíl mezi nimi je ten, že zatímco první příkaz používá specializované algoritmy pro řešení rovnic, druhý nejdrív spočte inverzi matice `A` a teprve potom ji vynásobí vektorem `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SparseArrays\n",
    "\n",
    "aux_s = 1000\n",
    "\n",
    "aux_A1 = sprandn(aux_s, aux_s, 0.001)\n",
    "aux_A1 += I\n",
    "aux_b = randn(aux_s);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: A1 not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: A1 not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[18]:1",
      " [2] include_string(::Function, ::Module, ::String, ::String) at .\\loading.jl:1091"
     ]
    }
   ],
   "source": [
    "aux_A2 = Matrix(A1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "norm(inv(aux_A2)*aux_b - aux_A2\\aux_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "@btime inv($aux_A2) * $aux_b;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime $aux_A2 \\ $aux_b;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = [0;0]\n",
    "\n",
    "x_opt2, x_all2, res2 = grad_descent(f_grad, x0)\n",
    "\n",
    "scatter(A[:,1], ys, label=\"Data\", legend=:topleft)\n",
    "plot!(xs, x -> x_opt2[1]*x + x_opt2[2], label=\"Fit\")\n",
    "\n",
    "create_anim(\"Anim_NC1.gif\", (x,y) -> f([x,y]), x_all2, xlim, ylim)\n",
    "\n",
    "# Newton-type methods\n",
    "\n",
    "f_hess_approx(x,λ) = g_grad(x)'*g_grad(x) / length(g(x)) + λ*Diagonal(ones(length(x)))\n",
    "\n",
    "ls_gauss_newton(f_grad, x0) = newton(f_grad, x -> f_hess_approx(x, 0), x0)\n",
    "ls_levenberg_marquardt(f_grad, x0, μ) = newton(f_grad, x -> f_hess_approx(x, μ), x0)\n",
    "\n",
    "x_opt3, x_all3, res3 = ls_gauss_newton(f_grad, x0)\n",
    "x_opt4, x_all4, res4 = ls_levenberg_marquardt(f_grad, x0, 0.5)\n",
    "\n",
    "plot(res4; yscale=:log10, label=\"Residual\")\n",
    "create_anim(\"Anim_NC2.gif\", (x,y) -> f([x,y]), x_all4, xlim, ylim)\n",
    "\n",
    "# Stochastic gradient descent\n",
    "\n",
    "g(x, I) = A[I,:]*x - ys[I]\n",
    "g_grad(x, I) = A[I,:]\n",
    "\n",
    "f(x, I) = g(x, I)'*g(x, I) / length(g(x, I))\n",
    "f_grad(x, I) = g_grad(x, I)'*g(x, I) / length(g(x, I))\n",
    "\n",
    "function stoch_grad_descent(grad, x, n; n_minibatch=8, α=1e-1, max_iter=100)\n",
    "    res1 = zeros(max_iter)\n",
    "    res2 = zeros(max_iter)\n",
    "    x_all = zeros(length(x), max_iter)\n",
    "    for i in 1:max_iter\n",
    "        x_all[:,i] = x\n",
    "        I = randperm(n)[1:n_minibatch]\n",
    "        x -= α*grad(x, I)\n",
    "        res1[i] = norm(grad(x, I))\n",
    "        res2[i] = norm(grad(x, 1:n))\n",
    "    end\n",
    "    return x, x_all, res1, res2\n",
    "end\n",
    "\n",
    "x_opt5, x_all5, res5_1, res5_2 = stoch_grad_descent(f_grad, x0, n)\n",
    "\n",
    "plot(res5_1; yscale=:log10, label=\"Residual minibatch\")\n",
    "plot!(res5_2; yscale=:log10, label=\"Residual true\")\n",
    "create_anim(\"Anim_NC3.gif\", (x,y) -> f([x,y]), x_all5, xlim, ylim)\n",
    "\n",
    "##################################\n",
    "### NELinearni nejmensi ctverce\n",
    "##################################\n",
    "\n",
    "# Non-linear regression (aka neural networks) - gradient descent\n",
    "\n",
    "using Flux\n",
    "using Flux: mse\n",
    "using Base.Iterators: partition\n",
    "\n",
    "xs_row = Float32.(reshape(xs,1,:))\n",
    "ys_row = Float32.(reshape(ys,1,:))\n",
    "\n",
    "n_hidden = 10\n",
    "m = Chain(\n",
    "    Dense(1, n_hidden, relu),\n",
    "    Dense(n_hidden, n_hidden, relu),\n",
    "    Dense(n_hidden, 1),\n",
    ")\n",
    "\n",
    "plot(xs, m(xs_row)[:], label=\"Initial neural network\")\n",
    "\n",
    "loss(x, y) = mse(m(x), y)\n",
    "\n",
    "ps = params(m)\n",
    "opt = Descent(1e-1)\n",
    "@time for i in 1:100\n",
    "    gs = gradient(ps) do\n",
    "        loss(xs_row, ys_row)\n",
    "    end\n",
    "\n",
    "    Flux.update!(opt, ps, gs)\n",
    "    println(loss(xs_row, ys_row))\n",
    "end\n",
    "\n",
    "plot(xs, ys, label=\"Data\", legend=:topleft)\n",
    "plot!(xs, m(xs_row)[:], label=\"Fit\")\n",
    "\n",
    "# Non-linear regression (aka neural networks) - stochastic gradient descent\n",
    "\n",
    "m = Chain(\n",
    "    Dense(1, n_hidden, relu),\n",
    "    Dense(n_hidden, n_hidden, relu),\n",
    "    Dense(n_hidden, 1),\n",
    ")\n",
    "\n",
    "batch_size = 10\n",
    "batches_train = map(partition(randperm(size(ys_row, 2)), batch_size)) do inds\n",
    "    return (xs_row[:, inds], ys_row[:, inds])\n",
    "end\n",
    "\n",
    "@time for _ in 1:100\n",
    "    Flux.train!(loss, params(m), batches_train, opt)\n",
    "    println(loss(xs_row, ys_row))\n",
    "end\n",
    "\n",
    "plot(xs, ys, label=\"Data\", legend=:topleft)\n",
    "plot!(xs, m(xs_row)[:], label=\"Fit\")\n",
    "\n",
    "\n",
    "mimo obor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n = 3000\n",
    "\n",
    "A = randn(n,n);\n",
    "b = randn(n);\n",
    "\n",
    "using BenchmarkTools\n",
    "\n",
    "@time A \\ b;\n",
    "@time inv(A) * b;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
